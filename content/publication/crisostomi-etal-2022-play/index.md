---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Play m√∫sica alegre: A Large-Scale Empirical Analysis of Cross-Lingual Phenomena
  in Voice Assistant Interactions'
subtitle: ''
summary: ''
authors:
- crisostomi
- Alessandro Manzotti
- Enrico Palumbo
- Davide Bernardi
- Sarah Campbell
- Shubham Garg
tags: []
categories: []
date: '2022-12-01'
lastmod: 2023-02-05T16:49:57+01:00
featured: false
draft: false
publication_short: "MMNLU workshop, EMNLP 2022"

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-05T15:50:39.853135Z'
publication_types:
- '1'
abstract: Cross-lingual phenomena are quite common in informal contexts like social
  media, where users are likely to mix their native language with English or other
  languages. However, few studies have focused so far on analyzing cross-lingual interactions
  in voice-assistant data, which present peculiar features in terms of sentence length,
  named entities, and use of spoken language. Also, little attention has been posed
  to European countries, where English is frequently used as a second language. In
  this paper, we present a large-scale empirical analysis of cross-lingual phenomena
  (code-mixing, linguistic borrowing, foreign named entities) in the interactions
  with a large-scale voice assistant in European countries. To do this, we first introduce
  a general, highly-scalable technique to generate synthetic mixed training data annotated
  with token-level language labels and we train two neural network models to predict
  them. We evaluate the models both on the synthetic dataset and on a real dataset
  of code-switched utterances, showing that the best performance is obtained by a
  character convolution based model. The results of the analysis highlight different
  behaviors between countries, having Italy with the highest ratio of cross-lingual
  utterances and Spain with a marked preference in keeping Spanish words. Our research,
  paired to the increase of the cross-lingual phenomena in time, motivates further
  research in developing multilingual Natural Language Understanding (NLU) models,
  which can naturally deal with cross-lingual interactions.
publication: '*Proceedings of the Massively Multilingual Natural Language Understanding
  Workshop (MMNLU-22)*'
links:
- icon: link
  icon_pack: fas
  name: 'URL'
  url: https://aclanthology.org/2022.mmnlu-1.5
---
